# BasisPilot 标签系统 — 命令使用指南

> 版本: v1.0
> 更新日期: 2026-02-21
> 前置条件: 题库已入库 (430+ 题), `data/taxonomy/` YAML 文件就位

---

## 1. 命令总览

```bash
# 所有命令的入口
python -m basis_expert_council.question_bank <command>
```

| 命令 | 说明 | 需要数据库 | 需要 LLM |
|------|------|:----------:|:--------:|
| `taxonomy` | 查看标签体系定义 | - | - |
| `taxonomy --validate` | 校验 YAML 文件完整性 | - | - |
| `tag --dry-run --limit N` | 试标注预览 (不写库) | Y | Y |
| `tag` | 全量 AI 批量打标 | Y | Y |
| `tag-stats` | 标签分布统计报告 | Y | - |

---

## 2. `taxonomy` — 查看/校验标签体系

### 2.1 查看概要

```bash
python -m basis_expert_council.question_bank taxonomy
```

输出示例:

```
标签体系 v1.0
  Bloom's 层级: 6 级
  DOK 层级: 4 级
  难度分段: 5 段
  认知技能: 5 大类, 26 子项
  情境类型: abstract, contextual, real_world, cross_disciplinary
  标签前缀: bloom, dok, band, ccss, skill, context, test, time, lang, mc
  CCSS 数学标准: 139 条
  CCSS 英语标准: 84 条
  易错点目录: 41 条
```

不需要数据库连接，仅加载本地 YAML 文件。

### 2.2 校验定义文件

```bash
python -m basis_expert_council.question_bank taxonomy --validate
```

校验内容:
- `taxonomy_v1.yaml` 存在且包含所有必要字段
- `ccss_math.yaml` 含 `standards` 字段
- `ccss_ela.yaml` 含 `standards` 字段
- `misconceptions.yaml` 含 `misconceptions` 字段
- 所有条目可被 Pydantic 模型成功解析

```
# 成功
标签体系校验通过! taxonomy_v1.yaml + ccss_math.yaml + ccss_ela.yaml + misconceptions.yaml

# 失败示例
标签体系校验失败! 2 个错误:
  - taxonomy_v1.yaml 缺少必要字段: blooms_taxonomy
  - misconceptions.yaml 解析失败: ...
```

---

## 3. `tag` — AI 批量打标

### 3.1 参数说明

```
usage: question_bank tag [-h] [--dry-run] [--limit LIMIT]
                         [--subject SUBJECT] [--grade GRADE]
                         [--batch-size BATCH_SIZE]

options:
  --dry-run             试运行，仅打印标签不写入数据库
  --limit LIMIT         限制处理题目数量 (默认: 全部)
  --subject SUBJECT     仅处理指定学科 (math/english)
  --grade GRADE         仅处理指定年级 (G5-G12)
  --batch-size SIZE     每批发送给 LLM 的题目数 (默认: 5)
```

### 3.2 试运行 (推荐首次使用)

```bash
# 预览 5 道题的标签结果
python -m basis_expert_council.question_bank tag --dry-run --limit 5
```

输出示例:

```
=== 试运行模式 (不写入数据库) ===
  进度: 5/5 题 (100%)

[DRY RUN] 题目 #42 (math/G7):
  Bloom's: {'level': 'apply', 'rationale': 'Requires applying ratio concepts'}
  DOK: {'level': 2, 'rationale': 'Skill/concept with one reasoning step'}
  CCSS: ['7.RP.A.2']
  Skills: {'primary': 'quantitative_reasoning.proportional_thinking', 'secondary': []}
  Tags: ['bloom:apply', 'dok:2', 'ccss:7.RP.A.2', 'skill:quantitative_reasoning.proportional_thinking', ...]

[DRY RUN] 题目 #43 (math/G7):
  ...

打标完成:
  总计: 5 题
  已标注: 5 题
  跳过 (已有标注): 0 题
```

### 3.3 按学科/年级打标

```bash
# 仅处理数学题
python -m basis_expert_council.question_bank tag --subject math

# 仅处理 G7 年级
python -m basis_expert_council.question_bank tag --grade G7

# 组合过滤
python -m basis_expert_council.question_bank tag --subject english --grade G8 --limit 20
```

### 3.4 全量打标

```bash
# 处理所有未打标题目 (430 题约 10-15 分钟)
python -m basis_expert_council.question_bank tag
```

输出示例:

```
  进度: 5/430 题 (1%)
  进度: 10/430 题 (2%)
  ...
  进度: 430/430 题 (100%)

打标完成:
  总计: 430 题
  已标注: 430 题
  跳过 (已有标注): 0 题
```

### 3.5 调整批大小

```bash
# 增大批次以减少 LLM 调用次数 (但 prompt 更长)
python -m basis_expert_council.question_bank tag --batch-size 10

# 减小批次以提高标注精度
python -m basis_expert_council.question_bank tag --batch-size 3
```

| batch-size | 430 题调用次数 | 优点 | 缺点 |
|:----------:|:-------------:|------|------|
| 3 | ~144 次 | 标注更精准 | 更慢、更贵 |
| 5 (默认) | ~86 次 | 速度/质量平衡 | — |
| 10 | ~43 次 | 最快 | prompt 较长，末尾题目质量可能下降 |

### 3.6 幂等机制

打标器通过 `metadata.taxonomy.version == "1.0"` 判断是否已打标:

```bash
# 第一次运行: 标注 430 题
python -m basis_expert_council.question_bank tag
# → 已标注: 430 题

# 第二次运行: 自动跳过
python -m basis_expert_council.question_bank tag
# → 总计: 0 题 (已全部标注)
```

如需**重新打标**，需要先清除 metadata 中的 taxonomy 字段:

```sql
-- 清除全部标签 (谨慎操作)
UPDATE assessment_questions
SET metadata = metadata - 'taxonomy', tags = '{}';

-- 清除单道题
UPDATE assessment_questions
SET metadata = metadata - 'taxonomy'
WHERE id = 42;
```

---

## 4. `tag-stats` — 标签分布统计

```bash
python -m basis_expert_council.question_bank tag-stats
```

输出示例:

```
========================================
        题目标签分布统计报告
========================================

概览:
  总题目数: 430
  已标注:   430 (100.0%)
  未标注:   0

Bloom's 认知层级分布:
  apply        : 180 (41.9%)
  understand   : 120 (27.9%)
  analyze      :  65 (15.1%)
  remember     :  40 ( 9.3%)
  evaluate     :  20 ( 4.7%)
  create       :   5 ( 1.2%)

Webb's DOK 分布:
  2            : 250 (58.1%)
  1            : 100 (23.3%)
  3            :  70 (16.3%)
  4            :  10 ( 2.3%)

难度分段分布:
  proficient   : 160 (37.2%)
  basic        : 120 (27.9%)
  advanced     :  80 (18.6%)
  foundational :  40 ( 9.3%)
  elite        :  30 ( 7.0%)

CCSS 标准码 TOP 10:
  7.RP.A.2     :  25
  6.EE.A.2     :  18
  7.NS.A.3     :  15
  ...

认知技能 TOP 10:
  quantitative_reasoning.proportional_thinking :  45
  quantitative_reasoning.number_sense          :  38
  reading_comprehension.inferential            :  32
  ...

易错点覆盖:
  MC-MATH-FRAC-001  :  12
  MC-MATH-RATIO-001 :   8
  MC-ENG-INF-001    :   6
  ...
```

---

## 5. 环境变量

| 变量 | 说明 | 默认值 |
|------|------|--------|
| `TAXONOMY_LLM_MODEL` | 打标使用的 LLM 模型 | (回退到 MEM0_LLM_MODEL) |
| `MEM0_LLM_MODEL` | 备选模型名 | `gpt-4o-mini` |
| `OPENAI_API_KEY` | OpenAI API 密钥 | (必填，用于 `tag` 命令) |
| `OPENAI_BASE_URL` | API 端点 | `https://api.openai.com/v1` |
| `BASIS_DATABASE_URL` | PostgreSQL 连接串 | `postgresql://postgres:postgres@localhost:5433/langgraph` |

**使用第三方兼容 API (如 DeepSeek / 智谱):**

```bash
export OPENAI_BASE_URL="https://api.deepseek.com/v1"
export OPENAI_API_KEY="sk-your-deepseek-key"
export TAXONOMY_LLM_MODEL="deepseek-chat"
python -m basis_expert_council.question_bank tag --dry-run --limit 5
```

---

## 6. 典型工作流

### 6.1 首次全量打标

```bash
# Step 1: 校验标签体系定义
python -m basis_expert_council.question_bank taxonomy --validate

# Step 2: 试标注 5 题确认效果
python -m basis_expert_council.question_bank tag --dry-run --limit 5

# Step 3: 小批量试跑 (写入 DB)
python -m basis_expert_council.question_bank tag --limit 20

# Step 4: 查看统计确认质量
python -m basis_expert_council.question_bank tag-stats

# Step 5: 全量打标
python -m basis_expert_council.question_bank tag

# Step 6: 最终统计
python -m basis_expert_council.question_bank tag-stats
```

### 6.2 新增题目后增量打标

```bash
# 导入新题目
python -m basis_expert_council.question_bank import new_questions.json

# 增量打标 (自动跳过已打标题目)
python -m basis_expert_council.question_bank tag

# 确认新题已标注
python -m basis_expert_council.question_bank tag-stats
```

### 6.3 按学科分批打标

```bash
# 数学先行
python -m basis_expert_council.question_bank tag --subject math
python -m basis_expert_council.question_bank tag-stats

# 英语跟进
python -m basis_expert_council.question_bank tag --subject english
python -m basis_expert_council.question_bank tag-stats
```

### 6.4 质量抽检

```bash
# 导出已打标的题目 (含 metadata.taxonomy)
python -m basis_expert_council.question_bank export --subject math --grade G7 -o g7_math.json

# 人工审核 JSON 中的 metadata.taxonomy 字段
# 确认 Bloom's/DOK 标注准确率 >85%
```

---

## 7. 标签写入格式

打标后每道题的数据库字段变化:

### `tags[]` 数组 (用于快速查询)

```
之前: ["ratio_proportion", "map_style"]
之后: ["bloom:apply", "dok:2", "band:proficient", "ccss:7.RP.A.2",
        "skill:quantitative_reasoning.proportional_thinking",
        "context:contextual", "test:map_growth", "time:standard",
        "mc:MC-MATH-FRAC-001"]
```

### `metadata.taxonomy` JSONB (完整结构化数据)

```json
{
  "taxonomy": {
    "version": "1.0",
    "tagged_at": "2026-02-21T14:30:00Z",
    "blooms": {"level": "apply", "rationale": "..."},
    "dok": {"level": 2, "rationale": "..."},
    "ccss_codes": ["7.RP.A.2"],
    "cognitive_skills": {"primary": "quantitative_reasoning.proportional_thinking", "secondary": []},
    "misconceptions": {"detectable": ["MC-MATH-FRAC-001"]},
    "learning_objective": "Apply unit rates to solve multi-step problems",
    "time_estimate_sec": 75,
    "context_type": "contextual",
    "test_alignment": ["map_growth"]
  }
}
```

---

## 8. SQL 查询速查

```sql
-- 查看打标覆盖率
SELECT
  COUNT(*) AS total,
  COUNT(*) FILTER (WHERE metadata->'taxonomy'->>'version' = '1.0') AS tagged
FROM assessment_questions;

-- 按 Bloom's 层级查询
SELECT * FROM assessment_questions WHERE 'bloom:apply' = ANY(tags);

-- 按 CCSS 标准查询
SELECT * FROM assessment_questions
WHERE EXISTS (SELECT 1 FROM unnest(tags) t WHERE t LIKE 'ccss:7.RP%');

-- 查找含特定易错点的题目
SELECT * FROM assessment_questions WHERE 'mc:MC-MATH-FRAC-001' = ANY(tags);

-- 按技能查询
SELECT * FROM assessment_questions
WHERE EXISTS (SELECT 1 FROM unnest(tags) t WHERE t LIKE 'skill:quantitative_reasoning%');

-- Bloom's 分布统计
SELECT
  t AS bloom_level,
  COUNT(*) AS cnt
FROM assessment_questions, unnest(tags) t
WHERE t LIKE 'bloom:%'
GROUP BY t ORDER BY cnt DESC;

-- 查看某道题的完整 taxonomy
SELECT id, subject, grade_level, topic, tags,
       metadata->'taxonomy' AS taxonomy
FROM assessment_questions WHERE id = 42;
```

---

## 9. 故障排查

| 问题 | 原因 | 解决方案 |
|------|------|---------|
| `ModuleNotFoundError: pyyaml` | 缺少 PyYAML 依赖 | `pip install pyyaml` |
| `ModuleNotFoundError: pydantic` | 缺少 Pydantic | `pip install pydantic` |
| `无法找到 data/taxonomy/ 目录` | 工作目录不在项目根 | `cd` 到项目根目录执行 |
| `LLM 调用失败` | API Key 未配置或余额不足 | 检查 `OPENAI_API_KEY` 和 `OPENAI_BASE_URL` |
| `总计: 0 题` | 所有题目已打标 | 幂等机制生效，无需重复打标 |
| 打标结果不理想 | 模型能力不足 | 尝试 `TAXONOMY_LLM_MODEL=gpt-4o` 或减小 `--batch-size` |
| 连接数据库失败 | PostgreSQL 未启动 | 检查 `BASIS_DATABASE_URL` 和 DB 服务状态 |

---

## 10. 注意事项

1. **费用预估**: 430 题 / batch_size=5 = 86 次 API 调用。使用 gpt-4o-mini 约 $0.5-1.0；使用 gpt-4o 约 $5-10
2. **断点续传**: 打标中断后重新运行即可，已打标题目会自动跳过
3. **并发安全**: 打标器顺序处理，不支持多实例同时打标同一批题目
4. **标签覆盖**: `tag` 命令会替换 `tags[]` 数组，旧的遗留标签不再保留。完整标签历史存储在 `metadata.taxonomy`
5. **版本升级**: 未来升级到 v2.0 时，修改 version 检查逻辑即可对已有题目重新打标
